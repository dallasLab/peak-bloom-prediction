---
title: "Peak Bloom Prediction Demo"
author: "Eager Learner"
date: "01/01/2022"
output:
  html_document:
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      out.width = '80%')
```

## Instructions

In this analysis we demonstrate a very simple way of predicting the peak bloom date in the coming decade for all four locations required by the competition.
The models used here are very simple and are using only the historic data for these four locations, but no other information or covariates.
At the end of this document ((#appendix-rnoaa)[Appendix A]), we also demonstrate a simple way to get historic temperature data for the four locations via the `rnoaa` package.


```{r}
library(tidyverse)
library(caret)
library(caretEnsemble)
library(doParallel)
library(foreach)

```


## Loading the data

The data for each of the three main sites is provided as simple text file in CSV format.
Each file contains the dates of the peak bloom of the cherry trees at the respective site, alongside the geographical location of the site.

The six columns in each data file are

* _location_ a human-readable location identifier (`string`).
* _lat_ (approximate) latitude of the cherry trees (`double`).
* _long_ (approximate) longitude of the cherry trees (`double`).
* _alt_ (approximate) altitude of the cherry trees (`double`).
* _year_ year of the observation (`integer`).
* *bloom_date* date of peak bloom of the cherry trees (ISO 8601 date `string`). The "peak bloom date" may be defined differently for different sites
* *bloom_doy* days since January 1st of the year until peak bloom (`integer`). January 1st corresponds to `1`.

In R, the data files can be read with `read.csv()` and concatenated with the `bind_rows()` function:

```{r}
# cherry <- read.csv("data/washingtondc.csv") %>% 
#   bind_rows(read.csv("data/liestal.csv")) %>% 
#   bind_rows(read.csv("data/kyoto.csv"))

cherry <- read.csv("data/InitCovariates.csv") %>%
  dplyr::select(-c("TmaxDaysSub0", "TmaxDaysSub5", "TmaxDaysSub10", "TmaxDaysSub20", "TmaxDaysSub25", "TminDaysSub0", "TminDaysSub5", "TminDaysSub10"))
```


## Visualizing the time series


```{r, fig.width=8, fig.height=3, out.width='100%', fig.cap="Time series of peak bloom of cherry trees since 1880 at three different sites."}
cherry %>% 
  filter(year >= 1880) %>%
  ggplot(aes(x = year, y = bloom_doy)) +
  geom_point() +
  geom_step(linetype = 'dotted', color = 'gray50') +
  scale_x_continuous(breaks = seq(1880, 2020, by = 20)) +
  facet_grid(cols = vars(str_to_title(location))) +
  labs(x = "Year", y = "Peak bloom (days since Jan 1st)")
```

```{r}
library(gbm)
library(dismo)

# remove irrelevant variable
cherry_temp <- cherry %>%
  dplyr::select(-c("bloom_date"))

# Separate into time based test and train data
cherry_test <- cherry_temp %>%
  filter(year >= 2010)

cherry_train <- cherry_temp %>%
  filter(year <2010)

# Set training parameters to 10-fold cross validation
trControl <- trainControl(method = "cv", number = 10)
```


```{r}
# Establish custom tuning grids for models 
tuneGridGBM <- expand.grid(interaction.depth = c(1,3,5),n.trees = (5:10)*100, shrinkage = c(.1,.01), n.minobsinnode =10)


```

```{r}

## Train a gbm model to look at variable selection


fullMod <- train(x = dplyr::select(cherry_train,-c("bloom_doy", "location")),
      y = cherry_train$bloom_doy,
      distribution = "gaussian",
      method="gbm",
      metric = "RMSE",
      tuneGrid = tuneGridGBM,
      trControl = trControl,
      verbose =F)

# Store variables ordered based on gbm importance
vars <- summary(fullMod$finalModel)$var

```


```{r Ensemble}
## Fit a single set of models to the training data and evaluate on the testing data


# Set up the models and settings for the full list of models
model_types <- list(gbm = caretModelSpec(method = "gbm", verbose =F, tuneGrid = tuneGridGBM),
                    rf = caretModelSpec(method = "rf"),
                    lasso= caretModelSpec(method = "lasso"),
                    ridge = caretModelSpec(method = "foba"),
                    xgBoost = caretModelSpec(method = "xgbTree", verbose =0),
                    svm = caretModelSpec(method = "svmLinear"),
                    enet = caretModelSpec(method = "enet"))
# Set up the list of just regularization focused models (no longer used)
model_types_reg <- list(
                    lasso= caretModelSpec(method = "lasso"),
                    ridge = caretModelSpec(method = "foba"),
                    enet = caretModelSpec(method = "enet"))

# Fit list of models
# Note that XGBtree will throw a bunch (A BUNCH) of warnings about deprecation of the ntree_limit argument
# This is the thing caret is tuning with and it does still seem to work as intended
# If we provide our own tuning grid we should use iteration_range....but for now I couldn't figure out how to suppress all the warnings.
# Apparently xgboost is notorious for warning suppression issues
mod_list <- caretList(
  x = dplyr::select(cherry_train,-c("bloom_doy", "location")),
  y = cherry_train$bloom_doy,
  trControl = trControl,
  tuneList = model_types
)


# Create stack of models combining predictions of component models based on an elastic net
mod_stack <- caretStack(
  mod_list,
  method = "enet"
)

# Create simple ensemble of models
mod_ensemble <- caretEnsemble(
  mod_list
)

# Produce predictions based on component and ensemble models for testing data
compPreds <- predict(mod_list, newdata = dplyr::select(cherry_test,-c("bloom_doy", "location")))
ensPreds <- predict(mod_ensemble, newdata = dplyr::select(cherry_test,-c("bloom_doy", "location")))
stackPreds <- predict(mod_stack, newdata = dplyr::select(cherry_test,-c("bloom_doy", "location")))


# Calculate RMSE for each model
rmseEns <- sqrt(mean((ensPreds-cherry_test$bloom_doy)^2))
rmseStack <- sqrt(mean((stackPreds-cherry_test$bloom_doy)^2))
rmseRF<- sqrt(mean((compPreds[,"rf"]-cherry_test$bloom_doy)^2))
rmseGBM<- sqrt(mean((compPreds[,"gbm"]-cherry_test$bloom_doy)^2))
rmseRidge<- sqrt(mean((compPreds[,"ridge"]-cherry_test$bloom_doy)^2))
rmseLasso<- sqrt(mean((compPreds[,"lasso"]-cherry_test$bloom_doy)^2))
rmseXGB<- sqrt(mean((compPreds[,"xgBoost"]-cherry_test$bloom_doy)^2))
rmseSVM<- sqrt(mean((compPreds[,"svm"]-cherry_test$bloom_doy)^2))
rmseENET<- sqrt(mean((compPreds[,"enet"]-cherry_test$bloom_doy)^2))
fullEnsPerf <- c(rmseEns, rmseStack, rmseRF, rmseGBM, rmseRidge, rmseLasso, rmseXGB, rmseSVM, rmseENET)

names(fullEnsPerf) <- c("Ens", "Stack", "RF", "GBM", "Ridge", "Lasso", "XGB", "SVM", "ENET")
```




```{r Variable Reduced Ensemble, eval =F}
## This old code iterates through a range of amount of parameter selection and saves the performance of the models
## It ranges from keeping just the top 5 variables to all
## It didn't really seem like there was an obvious cutoff based on earlier data
## Could be run again with new data to take a look
## Takes some time to run


subEnsPerf <- matrix(NA, nrow = 16, ncol = 9)

varIndex <- seq(5,35, by = 2)
for(i in 1:length(varIndex)){
cherry_test_sub <- cherry_test %>%
  dplyr::select(c(vars[1:varIndex[i]], "bloom_doy", "location"))

cherry_train_sub <- cherry_train %>%
  dplyr::select(c(vars[1:varIndex[i]], "bloom_doy", "location"))


mod_list <- caretList(
  x = dplyr::select(cherry_train_sub,-c("bloom_doy", "location")),
  y = cherry_train_sub$bloom_doy,
  trControl = trControl,
  methodList = c("gbm", "glm", "rf", "foba", "xgbTree", "svmLinear", "enet")
)


mod_stack <- caretStack(
  mod_list,
  method = "enet"
)

mod_ensemble <- caretEnsemble(
  mod_list
)

compPreds <- predict(mod_list, newdata = dplyr::select(cherry_test_sub,-c("bloom_doy", "location")))
ensPreds <- predict(mod_ensemble, newdata = dplyr::select(cherry_test_sub,-c("bloom_doy", "location")))
stackPreds <- predict(mod_stack, newdata = dplyr::select(cherry_test_sub,-c("bloom_doy", "location")))


rmseEns <- sqrt(mean((ensPreds-cherry_test_sub$bloom_doy)^2))
rmseStack <- sqrt(mean((stackPreds-cherry_test_sub$bloom_doy)^2))
rmseGLM <- sqrt(mean((compPreds[,"glm"]-cherry_test_sub$bloom_doy)^2))
rmseRF<- sqrt(mean((compPreds[,"rf"]-cherry_test_sub$bloom_doy)^2))
rmseGBM<- sqrt(mean((compPreds[,"gbm"]-cherry_test_sub$bloom_doy)^2))
rmseFOBA<- sqrt(mean((compPreds[,"foba"]-cherry_test_sub$bloom_doy)^2))
rmseXGB<- sqrt(mean((compPreds[,"xgbTree"]-cherry_test_sub$bloom_doy)^2))
rmseSVM<- sqrt(mean((compPreds[,"svmLinear"]-cherry_test_sub$bloom_doy)^2))
rmseENET<- sqrt(mean((compPreds[,"enet"]-cherry_test_sub$bloom_doy)^2))

subEnsPerf[i,] <- c(rmseEns, rmseStack, rmseGLM, rmseRF, rmseGBM, rmseFOBA, rmseXGB, rmseSVM, rmseENET)

}
colnames(subEnsPerf) <- c("Ens", "Stack","GLM", "RF", "GBM", "FOBA", "XGB", "SVM", "ENET")

```


```{r evaluate on next year for 10 years}
# This script provides a method of evaluation more relevant to our use case
# It trains the model on all data up until a given year in our testing set (including any earlier data in the testing set)
# Then is evaluated on just that next testing year
# We loop through all years in the testing set except for 2015 because we don't have that row for switzerland

# Set up the year range
evalYears <- 2010:2021
evalYears <- evalYears[-6] # Get rid of 2015 because it's missing in switzerland

# Initialize matrix to store performance
ensOneYearPerf <- matrix(NA, nrow = length(evalYears), ncol = 10)

# Loop over years
for(i in 1:length(evalYears)){
  # Subset testing data to test year
  cherry_test_plus <- filter(cherry_test, year ==evalYears[i])
  # Add test data prior to test year to training data
  cherry_train_plus <- bind_rows(cherry_train,
                                 filter(cherry_test, year < evalYears[i]))
  
  set.seed(8675309)
  # Fit full model list
  mod_list <- caretList(
    x = dplyr::select(cherry_train_plus,-c("bloom_doy", "location")),
    y = cherry_train_plus$bloom_doy,
    trControl = trControl,
    tuneList = model_types
    )
  set.seed(8675309)
  # Fit model list with just regularization models. You can't pull component models out of a caretList object and ensemble just them
  # So this is how you have to do it
  # This also means that the ensemble with base models based on different datasets doesn't seem to work with this packages. Which is dumb.
  mod_list_reg <- caretList(
    x = dplyr::select(cherry_train_plus,-c("bloom_doy", "location")),
    y = cherry_train_plus$bloom_doy,
    trControl = trControl,
    tuneList = model_types_reg
    )
  
  # Produce the stack/ensemble models
  
  mod_stack_reg <- caretStack(
    mod_list_reg,
    method = "enet"
  )
  
  mod_stack<- caretStack(
    mod_list,
    method = "enet"
  )
  
  mod_ensemble <- caretEnsemble(
    mod_list
  )
  
  # Get predictions of all models for the testing year
  compPreds <- predict(mod_list, newdata = dplyr::select(cherry_test_plus,-c("bloom_doy", "location")))
  ensPreds <- predict(mod_ensemble, newdata = dplyr::select(cherry_test_plus,-c("bloom_doy", "location")))
  stackPreds <- predict(mod_stack, newdata = dplyr::select(cherry_test_plus,-c("bloom_doy", "location")))
  stackRegPreds <-  predict(mod_stack_reg, newdata = dplyr::select(cherry_test_plus,-c("bloom_doy", "location")))
  
  # Calculate RMSE for each model type
  # Should probably write a function for this
  rmseEns <- sqrt(mean((ensPreds-cherry_test_plus$bloom_doy)^2))
  rmseStack <- sqrt(mean((stackPreds-cherry_test_plus$bloom_doy)^2))
  rmseStackReg <- sqrt(mean((stackRegPreds-cherry_test_plus$bloom_doy)^2))
  rmseRF<- sqrt(mean((compPreds[,"rf"]-cherry_test_plus$bloom_doy)^2))
  rmseGBM<- sqrt(mean((compPreds[,"gbm"]-cherry_test_plus$bloom_doy)^2))
  rmseRidge<- sqrt(mean((compPreds[,"ridge"]-cherry_test_plus$bloom_doy)^2))
  rmseLasso<- sqrt(mean((compPreds[,"lasso"]-cherry_test_plus$bloom_doy)^2))
  rmseXGB<- sqrt(mean((compPreds[,"xgBoost"]-cherry_test_plus$bloom_doy)^2))
  rmseSVM<- sqrt(mean((compPreds[,"svm"]-cherry_test_plus$bloom_doy)^2))
  rmseENET<- sqrt(mean((compPreds[,"enet"]-cherry_test_plus$bloom_doy)^2))
  
  ensOneYearPerf[i,] <- c(rmseEns, rmseStack, rmseStackReg,rmseRF, rmseGBM, rmseRidge, rmseLasso, rmseXGB, rmseSVM, rmseENET)
  
}

colnames(ensOneYearPerf) <- c("Ens", "Stack","StackReg", "RF", "GBM", "RIDGE", "LASSO", "XGB", "SVM", "ENET")

```

```{r Variable importance based on 1 year at a time}

# This code measures variable importance through a modified permutation approach based on the full stacked model
# It should be easy to adapt if we choose a different model
# I say "modified" because the nature of our evaluation (the NEXT year) makes permutation a sort of wonky thing to do. It doesn't seem reasonable to just mess up the given variable for that year and those 3 locations and call it a day. Because of climate change etc.
# Instead we are going to pick a random other value for the variable from the training data for that variable

# Set up the year range
evalYears <- 2010:2021
evalYears <- evalYears[-6] # Get rid of 2015 because it's missing in switzerland

# Get all the variable names except for location and bloom_doy
varNames <- names(cherry_train)[-c(1,6)]
# Initialize matrix to store performance
permuteOneYearPerf <- matrix(NA, nrow = length(evalYears), ncol = length(varNames))

set.seed(8675309)
tic()
# Loop over years using a foreach statement
for(i in  1:length(evalYears)){
  varPerf <- c()
  for(j in 1:length(varNames)){
    # Repeat the permutation on each variable 10 times because the sample size is small.
    
    # Add test data prior to test year to training data
    cherry_train_plus <- bind_rows(cherry_train,
                                 filter(cherry_test, year < evalYears[i]))
    
    # Fit full model list we can do this outside the below loop because that loop only affects testing data
    mod_list <- caretList(
      x = dplyr::select(cherry_train_plus,-c("bloom_doy", "location")),
      y = cherry_train_plus$bloom_doy,
      trControl = trControl,
      tuneList = model_types
    )
    
    # Produce the stack/ensemble models
    
    
    mod_stack<- caretStack(
      mod_list,
      method = "enet"
    )
    tic()
    rmseStack <- c()
    for(k in 1:10){
    # Subset testing data to test year
    cherry_test_plus <- filter(cherry_test, year ==evalYears[i])
    # Replace values of chosen variable in the testing data with random values of that variable from training data
    # This should remove the relationship (on average) without causing too many theor
    cherry_test_plus[,varNames[j]] <- sample(c(cherry_train_plus[,varNames[j]]), size = 3)
    
    
    # Get predictions of all models for the testing year
    stackPreds <- predict(mod_stack, newdata = dplyr::select(cherry_test_plus,-c("bloom_doy", "location")))
    
    # Calculate RMSE for each model type
    # Should probably write a function for this
    rmseStack[k] <- sqrt(mean((stackPreds-cherry_test_plus$bloom_doy)^2))
    }
    toc()
    # Store the mean performance across the 10 permutations of that variable
    varPerf[j] <- mean(rmseStack)
  }
  # Worker node returns this vector and then binds it as a row to the others
  permuteOneYearPerf[i,j] <- varPerf
}


colnames(permuteOneYearPerf) <- vars

# Difference in performance between permuted and unpermuted variable models
perfDifference <- permuteOneYearPerf-ensOneYearPerf

# Mean of these across years as a proxy for influence
meanInfluence <- colMeans(perfDifference)
toc()

```


```{r Variable importance based on all testing years}

# This code measures variable importance through a modified permutation approach based on the full stacked model
# It should be easy to adapt if we choose a different model
# I say "modified" because the nature of our evaluation (the NEXT year) makes permutation a sort of wonky thing to do. It doesn't seem reasonable to just mess up the given variable for that year and those 3 locations and call it a day. Because of climate change etc.
# Instead we are going to pick a random other value for the variable from the training data for that variable

set.seed(8675309)
tic()
# Loop over years using a foreach statement
varPerf <- c()
for(j in 1:length(varNames)){
  # Repeat the permutation on each variable 10 times because the sample size is small.
  
  # Fit full model list we can do this outside the below loop because that loop only affects testing data
  mod_list <- caretList(
    x = dplyr::select(cherry_train,-c("bloom_doy", "location")),
    y = cherry_train$bloom_doy,
    trControl = trControl,
    tuneList = model_types
  )
  
  # Produce the stack/ensemble models
  
  
  mod_stack<- caretStack(
    mod_list,
    method = "enet"
  )
  # Replace values of chosen variable in the testing data with random values of that variable from training data
  # This should remove the relationship (on average) without causing too many theor
  cherry_test[,varNames[j]] <- sample(c(cherry_test[,varNames[j]]), size = nrow(cherry_test))
  
  
  # Get predictions of all models for the testing year
  stackPreds <- predict(mod_stack, newdata = dplyr::select(cherry_test,-c("bloom_doy", "location")))
  
  # Calculate RMSE for each model type
  # Should probably write a function for this
  rmseStack <- sqrt(mean((stackPreds-cherry_test$bloom_doy)^2))
  # Store the mean performance across the 10 permutations of that variable
  varPerf[j] <- mean(rmseStack)
}
toc()


# Difference in performance between permuted and unpermuted variable models
perfDifference <- permuteOneYearPerf-ensOneYearPerf

# Mean of these across years as a proxy for influence
meanInfluence <- colMeans(perfDifference)
toc()

```